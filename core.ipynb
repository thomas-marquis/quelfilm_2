{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import functools\n",
    "import copy\n",
    "import typing as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Convolution1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading french: Package 'french' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_for_each_key(data: dict, func: t.Callable) -> dict:\n",
    "    return {k:func(data[k]) for k in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def load(self) -> dict:\n",
    "        raise Exception('load method not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputData:\n",
    "    examples: t.Dict[str, t.List[str]]\n",
    "    responses: t.Dict[str, t.List[str]]\n",
    "        \n",
    "    def __init__(self, examples, responses):\n",
    "        self.examples = examples\n",
    "        self.responses = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MdLoader(DataLoader):\n",
    "    path: str\n",
    "        \n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "        \n",
    "    def get_if_match(self, regex, text):\n",
    "        matcher = re.match(regex, text)\n",
    "        if matcher is None:\n",
    "            return None\n",
    "        return matcher.group(1)\n",
    "        \n",
    "    def get_list_item(self, line):\n",
    "        return self.get_if_match(r'- (.*)$', line)\n",
    "    \n",
    "    def get_intent_name(self, line):\n",
    "        return self.get_if_match(r'^## (.*)$', line)\n",
    "    \n",
    "    def get_sub_part(self, line: str) -> str:\n",
    "        return self.get_if_match(r'^### (.*)$', line)\n",
    "    \n",
    "    def load(self) -> InputData:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            content = [re.sub('\\\\n', '', l) for l in file]    \n",
    "        mega_part = ''\n",
    "        current_intent = ''\n",
    "        current_sub_part = ''\n",
    "        intents = dict()\n",
    "        response = dict()\n",
    "        \n",
    "        for line in content:\n",
    "            if line == '# intents':\n",
    "                mega_part = 'intents'\n",
    "            if mega_part == 'intents':\n",
    "                intent_name: str = self.get_intent_name(line)\n",
    "                list_item: str = self.get_list_item(line)\n",
    "                sub_part: str = self.get_sub_part(line)\n",
    "                    \n",
    "                if intent_name is not None and intent_name != '':\n",
    "                    current_intent = intent_name\n",
    "                    if intent_name != 'not_found':\n",
    "                        intents[intent_name] =  []\n",
    "                    response[intent_name] = []\n",
    "                    \n",
    "                if sub_part is not None:\n",
    "                    current_sub_part = sub_part\n",
    "                \n",
    "                if current_sub_part == 'examples' and list_item is not None:\n",
    "                    if current_intent != 'not_found':\n",
    "                        intents[current_intent].append(list_item)\n",
    "                \n",
    "                if current_sub_part == 'response' and list_item is not None:\n",
    "                    response[current_intent].append(list_item)\n",
    "                    \n",
    "        return InputData(intents, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, data_loader: DataLoader):\n",
    "        self.tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        self.stop_words = nltk.corpus.stopwords.words('french')\n",
    "        self.stemmer = FrenchStemmer()\n",
    "        \n",
    "        # TODO compose it\n",
    "        inputs: InputData = data_loader.load()\n",
    "        data = inputs.examples\n",
    "        self.responses = inputs.responses\n",
    "        data = self.to_lower_case_all(data)\n",
    "        data = self.tokenize_all_examples(data)\n",
    "        data = self.remove_stop_words_for_all(data)\n",
    "        data = self.lemmatize_all(data)\n",
    "        self.data = data\n",
    "        \n",
    "    def to_lower_case_one(self, example: str):\n",
    "        return example.lower()\n",
    "        \n",
    "    def to_lower_case_all(self, data):\n",
    "        to_lower_case = lambda examples: [self.to_lower_case_one(ex) for ex in examples]\n",
    "        return apply_for_each_key(data, to_lower_case)\n",
    "    \n",
    "    def tokenize_one_example(self, example):\n",
    "        return self.tokenizer.tokenize(example)\n",
    "    \n",
    "    def tokenize_all_examples(self, data):\n",
    "        tokenize = lambda data_list: [self.tokenize_one_example(d) for d in data_list]\n",
    "        return apply_for_each_key(data, tokenize)\n",
    "    \n",
    "    def remove_stop_words(self, example):\n",
    "        return [w for w in example if not w in self.stop_words]\n",
    "    \n",
    "    def remove_stop_words_for_all(self, data):\n",
    "        remove_sw = lambda examples: [self.remove_stop_words(ex) for ex in examples]\n",
    "        return apply_for_each_key(data, remove_sw)\n",
    "    \n",
    "    def lemmatize(self, example):\n",
    "        return [self.stemmer.stem(w) for w in example]\n",
    "    \n",
    "    def lemmatize_all(self, data):\n",
    "        get_lems = lambda examples: [self.lemmatize(ex) for ex in examples]\n",
    "        return apply_for_each_key(data, get_lems)\n",
    "    \n",
    "    def process_sentence(self, sentence):\n",
    "        # TODO compose\n",
    "        data = self.to_lower_case_one(sentence)\n",
    "        data = self.tokenize_one_example(data)\n",
    "        data = self.remove_stop_words(data)\n",
    "        data = self.lemmatize(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedRepresentation:\n",
    "    index_columns: t.List[str] = ['lem', 'index']\n",
    "    index: pd.DataFrame\n",
    "    index_len: int\n",
    "    data: t.Dict[str, np.array]\n",
    "    data_merged: t.Dict[str, np.array]\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.index = self.build_index(data)\n",
    "        self.index_len = self.index.shape[0]\n",
    "        \n",
    "    def build_index(self, data) -> pd.DataFrame:\n",
    "        index = dict()\n",
    "        idx = 0\n",
    "        for intent in data:\n",
    "            for example in data[intent]:\n",
    "                for lem in example:\n",
    "                    if index.get(lem) is None:\n",
    "                        index[lem] = idx\n",
    "                        idx += 1\n",
    "        return pd.DataFrame([{'lem': k, 'index': index[k]} for k in index], columns=self.index_columns)\n",
    "    \n",
    "    def get_index_from_lem(self, lem) -> int:\n",
    "        idx = self.index.loc[self.index['lem'] == lem, 'index']\n",
    "        if idx.empty:\n",
    "            return None\n",
    "        else:\n",
    "            return idx.values[0]\n",
    "        \n",
    "    def get_matrix_from_index(self, idx) -> np.array:\n",
    "        matrix = np.zeros(self.index_len)\n",
    "        matrix[idx] = 1\n",
    "        return matrix\n",
    "    \n",
    "    def get_matrix_from_lem(self, lem) -> np.array:\n",
    "        idx = self.get_index_from_lem(lem)\n",
    "        if idx is None:\n",
    "            return idx\n",
    "        else:\n",
    "            return self.get_matrix_from_index(idx)\n",
    "    \n",
    "    def process_words_matrix(self, example) -> np.array:\n",
    "        return np.array(\n",
    "            [self.get_matrix_from_lem(l) for l in example if not self.get_matrix_from_lem(l) is None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixRepresentation(IndexedRepresentation):\n",
    "    data: t.Dict[str, np.array]\n",
    "        \n",
    "    def __init__(self, data):\n",
    "        IndexedRepresentation.__init__(self, data)\n",
    "        self.data = self.process_words_matrix_for_all(data)\n",
    "    \n",
    "    def process_words_matrix_for_all(self, data) -> t.Dict[str, np.array]:\n",
    "        get_matrix = lambda examples: np.array(\n",
    "            [self.process_words_matrix(ex) for ex in examples])\n",
    "        return apply_for_each_key(data, get_matrix)\n",
    "    \n",
    "    def process_new_data(self, lems) -> np.array:\n",
    "        return self.process_words_matrix(lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedMatrixRepresentation(IndexedRepresentation):\n",
    "    data: t.Dict[str, np.array]\n",
    "        \n",
    "    def __init__(self, data):\n",
    "        IndexedRepresentation.__init__(self, data)\n",
    "        self.data = self.process_merged_sentences_for_all(data)\n",
    "        \n",
    "    def process_words_merged_matrix(self, example) -> np.array:\n",
    "        matrix = self.process_words_matrix(example)\n",
    "        return sum(list(matrix))\n",
    "    \n",
    "    def process_merged_sentences_for_all(self, data) -> t.Dict[str, np.array]:\n",
    "        get_matrix = lambda examples: np.array(\n",
    "            [self.process_words_merged_matrix(ex) for ex in examples])\n",
    "        return apply_for_each_key(data, get_matrix)\n",
    "    \n",
    "    def process_new_data(self, lems) -> np.array:\n",
    "        return self.process_words_merged_matrix(lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractClassifier:\n",
    "    def build(self, data):\n",
    "        raise Exception('build mathod not implemented')\n",
    "        \n",
    "    def train(self):\n",
    "        raise Exception('train mathod not implemented')\n",
    "        \n",
    "    def predict(self, value):\n",
    "        raise Exception('predict method not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionClassifier(AbstractClassifier):\n",
    "    data: t.Dict[str, np.array]\n",
    "    input_shape: t.Tuple[int, int]\n",
    "    model: Sequential\n",
    "    \n",
    "    def __init__(self, input_shape: t.Tuple[int, int]):\n",
    "        AbstractClassifier.__init__(self)\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def build(self, data):\n",
    "        self.data = data\n",
    "        self.model = Sequential([\n",
    "            Convolution1D(\n",
    "                filters=32, \n",
    "                kernel_size=3, \n",
    "                activation='relu', \n",
    "                input_shape=(self.input_shape)),\n",
    "            MaxPooling1D(),\n",
    "            Dense(units=100)\n",
    "        ])\n",
    "        \n",
    "    def train(self):\n",
    "        return 'train'\n",
    "    \n",
    "    def predict(self, value):\n",
    "        return 'predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationProcessor:\n",
    "    training_set: t.Dict[str, np.array]\n",
    "    classifier: AbstractClassifier\n",
    "    \n",
    "    def __init__(self, \n",
    "                 classifier: AbstractClassifier, \n",
    "                 training_set: t.Dict[str, np.array]):\n",
    "        self.training_set = training_set\n",
    "        self.classifier = classifier\n",
    "        self.classifier.build(training_set)\n",
    "        \n",
    "    def train(self):\n",
    "        classifier.train()\n",
    "    \n",
    "    def predict(self, data: np.array):\n",
    "        return classifier.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayseClassifier(AbstractClassifier):\n",
    "    \"\"\"besoin de la représentation mergée des exemples\n",
    "    \"\"\"\n",
    "    data: t.Dict[str, np.array]\n",
    "    proba_per_intent: t.Dict[str, np.array]\n",
    "        \n",
    "    def build(self, data: t.Dict[str, np.array]):\n",
    "        self.data = data\n",
    "    \n",
    "    def get_proba_per_intent(self, examples):\n",
    "        return sum(list(examples)) / len(examples)\n",
    "    \n",
    "    def train(self):\n",
    "        self.proba_per_intent: t.Dict[str, np.array] = \\\n",
    "            {k:self.get_proba_per_intent(self.data[k]) for k in self.data}\n",
    "    \n",
    "    def predict(self, value: np.array) -> t.Tuple[str, float]:\n",
    "        max_p = 0\n",
    "        intents = list(self.data.keys())\n",
    "        best_intent = 'not_found'\n",
    "        for intent in intents:\n",
    "            repres = value * self.proba_per_intent[intent]\n",
    "            repres = [e for e in repres if e != 0]\n",
    "            current_p = 0\n",
    "            if len(repres) != 0:\n",
    "                current_p = np.prod(repres)\n",
    "            if current_p > max_p:\n",
    "                max_p = current_p\n",
    "                best_intent = intent\n",
    "        return best_intent, max_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MdLoader('./training.md')\n",
    "processor = Preprocessing(loader)\n",
    "merged_matrix_repr = MergedMatrixRepresentation(processor.data)\n",
    "\n",
    "classifier = NaiveBayseClassifier()\n",
    "classifier_processor = ClassificationProcessor(classifier, merged_matrix_repr.data)\n",
    "classifier_processor.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_message(messages: t.List[str]) -> str:\n",
    "    i = np.random.randint(low=0, high=len(messages))\n",
    "    return messages[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"j'en sais rien moi !\""
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"j'aimerai voir un bon film\"\n",
    "message = processor.process_sentence(message)\n",
    "message = merged_matrix_repr.process_new_data(message)\n",
    "res = classifier_processor.predict(message)\n",
    "get_random_message(processor.responses[res[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
